"""
Zepsearchtoolservice
å°è£…å›¾è°±searchã€nodesreadã€edgesqueryetctoolï¼Œä¾›Report Agentuse

æ ¸å¿ƒsearchtoolï¼ˆä¼˜åŒ–åï¼‰ï¼š
1. InsightForgeï¼ˆDeepinsightsearchï¼‰- æœ€å¼ºå¤§ofhybridsearchï¼Œè‡ªåŠ¨generateå­é—®é¢˜å¹¶å¤šç»´åº¦search
2. PanoramaSearchï¼ˆå¹¿åº¦searchï¼‰- getå…¨è²Œï¼Œpackageæ‹¬expiredcontent
3. QuickSearchï¼ˆç®€å•searchï¼‰- quicksearch
"""

import time
import json
from typing import Dict, Any, List, Optional
from dataclasses import dataclass, field

from zep_cloud.client import Zep

from ..config import Config
from ..utils.logger import get_logger
from ..utils.llm_client import LLMClient

logger = get_logger('mirofish.zep_tools')


@dataclass
class SearchResult:
    """searchresult"""
    facts: List[str]
    edges: List[Dict[str, Any]]
    nodes: List[Dict[str, Any]]
    query: str
    total_count: int
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "facts": self.facts,
            "edges": self.edges,
            "nodes": self.nodes,
            "query": self.query,
            "total_count": self.total_count
        }
    
    def to_text(self) -> str:
        """Convertforæ–‡æœ¬formatï¼Œä¾›LLMç†è§£"""
        text_parts = [f"searchquery: {self.query}", f"æ‰¾åˆ° {self.total_count} relatedinformation"]
        
        if self.facts:
            text_parts.append("\n### relatedfacts:")
            for i, fact in enumerate(self.facts, 1):
                text_parts.append(f"{i}. {fact}")
        
        return "\n".join(text_parts)


@dataclass
class NodeInfo:
    """nodeinformation"""
    uuid: str
    name: str
    labels: List[str]
    summary: str
    attributes: Dict[str, Any]
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "uuid": self.uuid,
            "name": self.name,
            "labels": self.labels,
            "summary": self.summary,
            "attributes": self.attributes
        }
    
    def to_text(self) -> str:
        """Convertforæ–‡æœ¬format"""
        entity_type = next((l for l in self.labels if l not in ["Entity", "Node"]), "notçŸ¥type")
        return f"entity: {self.name} (type: {entity_type})\næ‘˜want: {self.summary}"


@dataclass
class EdgeInfo:
    """edgeinformation"""
    uuid: str
    name: str
    fact: str
    source_node_uuid: str
    target_node_uuid: str
    source_node_name: Optional[str] = None
    target_node_name: Optional[str] = None
    # timeinformation
    created_at: Optional[str] = None
    valid_at: Optional[str] = None
    invalid_at: Optional[str] = None
    expired_at: Optional[str] = None
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "uuid": self.uuid,
            "name": self.name,
            "fact": self.fact,
            "source_node_uuid": self.source_node_uuid,
            "target_node_uuid": self.target_node_uuid,
            "source_node_name": self.source_node_name,
            "target_node_name": self.target_node_name,
            "created_at": self.created_at,
            "valid_at": self.valid_at,
            "invalid_at": self.invalid_at,
            "expired_at": self.expired_at
        }
    
    def to_text(self, include_temporal: bool = False) -> str:
        """Convertforæ–‡æœ¬format"""
        source = self.source_node_name or self.source_node_uuid[:8]
        target = self.target_node_name or self.target_node_uuid[:8]
        base_text = f"relationship: {source} --[{self.name}]--> {target}\nfacts: {self.fact}"
        
        if include_temporal:
            valid_at = self.valid_at or "notçŸ¥"
            invalid_at = self.invalid_at or "è‡³ä»Š"
            base_text += f"\næ—¶æ•ˆ: {valid_at} - {invalid_at}"
            if self.expired_at:
                base_text += f" (alreadyexpired: {self.expired_at})"
        
        return base_text
    
    @property
    def is_expired(self) -> bool:
        """whether toalreadyexpired"""
        return self.expired_at is not None
    
    @property
    def is_invalid(self) -> bool:
        """whether toalreadyå¤±æ•ˆ"""
        return self.invalid_at is not None


@dataclass
class InsightForgeResult:
    """
    Deepinsightsearchresult (InsightForge)
    containså¤šå­é—®é¢˜ofsearchresultï¼Œandç»¼åˆåˆ†æ
    """
    query: str
    simulation_requirement: str
    sub_queries: List[str]
    
    # å„ç»´åº¦retrievalresult
    semantic_facts: List[str] = field(default_factory=list)  # è¯­ä¹‰searchresult
    entity_insights: List[Dict[str, Any]] = field(default_factory=list)  # entityinsight
    relationship_chains: List[str] = field(default_factory=list)  # relationshipé“¾
    
    # statisticsinformation
    total_facts: int = 0
    total_entities: int = 0
    total_relationships: int = 0
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "query": self.query,
            "simulation_requirement": self.simulation_requirement,
            "sub_queries": self.sub_queries,
            "semantic_facts": self.semantic_facts,
            "entity_insights": self.entity_insights,
            "relationship_chains": self.relationship_chains,
            "total_facts": self.total_facts,
            "total_entities": self.total_entities,
            "total_relationships": self.total_relationships
        }
    
    def to_text(self) -> str:
        """Convertfordetailedofæ–‡æœ¬formatï¼Œä¾›LLMç†è§£"""
        text_parts = [
            f"## notcomeé¢„æµ‹Deepåˆ†æ",
            f"åˆ†æé—®é¢˜: {self.query}",
            f"é¢„æµ‹åœºæ™¯: {self.simulation_requirement}",
            f"\n### é¢„æµ‹countæ®statistics",
            f"- relatedé¢„æµ‹facts: {self.total_facts}",
            f"- æ¶‰andentity: {self.total_entities}",
            f"- relationshipé“¾: {self.total_relationships}"
        ]
        
        # å­é—®é¢˜
        if self.sub_queries:
            text_parts.append(f"\n### åˆ†æofå­é—®é¢˜")
            for i, sq in enumerate(self.sub_queries, 1):
                text_parts.append(f"{i}. {sq}")
        
        # è¯­ä¹‰searchresult
        if self.semantic_facts:
            text_parts.append(f"\n### ã€å…³keyfactsã€‘(è¯·inreport å¼•usethisäº›åŸæ–‡)")
            for i, fact in enumerate(self.semantic_facts, 1):
                text_parts.append(f"{i}. \"{fact}\"")
        
        # entityinsight
        if self.entity_insights:
            text_parts.append(f"\n### ã€æ ¸å¿ƒentityã€‘")
            for entity in self.entity_insights:
                text_parts.append(f"- **{entity.get('name', 'notçŸ¥')}** ({entity.get('type', 'entity')})")
                if entity.get('summary'):
                    text_parts.append(f"  æ‘˜want: \"{entity.get('summary')}\"")
                if entity.get('related_facts'):
                    text_parts.append(f"  relatedfacts: {len(entity.get('related_facts', []))}")
        
        # relationshipé“¾
        if self.relationship_chains:
            text_parts.append(f"\n### ã€relationshipé“¾ã€‘")
            for chain in self.relationship_chains:
                text_parts.append(f"- {chain}")
        
        return "\n".join(text_parts)


@dataclass
class PanoramaResult:
    """
    å¹¿åº¦searchresult (Panorama)
    containsæ‰€haverelatedinformationï¼Œpackageæ‹¬expiredcontent
    """
    query: str
    
    # å…¨éƒ¨node
    all_nodes: List[NodeInfo] = field(default_factory=list)
    # å…¨éƒ¨edgeï¼ˆpackageæ‹¬expiredofï¼‰
    all_edges: List[EdgeInfo] = field(default_factory=list)
    # Currenthaveæ•ˆoffacts
    active_facts: List[str] = field(default_factory=list)
    # alreadyexpired/å¤±æ•ˆoffactsï¼ˆhistoryrecordï¼‰
    historical_facts: List[str] = field(default_factory=list)
    
    # statistics
    total_nodes: int = 0
    total_edges: int = 0
    active_count: int = 0
    historical_count: int = 0
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "query": self.query,
            "all_nodes": [n.to_dict() for n in self.all_nodes],
            "all_edges": [e.to_dict() for e in self.all_edges],
            "active_facts": self.active_facts,
            "historical_facts": self.historical_facts,
            "total_nodes": self.total_nodes,
            "total_edges": self.total_edges,
            "active_count": self.active_count,
            "historical_count": self.historical_count
        }
    
    def to_text(self) -> str:
        """Convertforæ–‡æœ¬formatï¼ˆcompleteç‰ˆæœ¬ï¼Œnotæˆªæ–­ï¼‰"""
        text_parts = [
            f"## å¹¿åº¦searchresultï¼ˆnotcomepanoramaviewï¼‰",
            f"query: {self.query}",
            f"\n### statisticsinformation",
            f"- Totalnodecount: {self.total_nodes}",
            f"- Totaledgecount: {self.total_edges}",
            f"- Currenthaveæ•ˆfacts: {self.active_count}",
            f"- history/expiredfacts: {self.historical_count}"
        ]
        
        # Currenthaveæ•ˆoffactsï¼ˆcompleteè¾“å‡ºï¼Œnotæˆªæ–­ï¼‰
        if self.active_facts:
            text_parts.append(f"\n### ã€Currenthaveæ•ˆfactsã€‘(simulationresultåŸæ–‡)")
            for i, fact in enumerate(self.active_facts, 1):
                text_parts.append(f"{i}. \"{fact}\"")
        
        # history/expiredfactsï¼ˆcompleteè¾“å‡ºï¼Œnotæˆªæ–­ï¼‰
        if self.historical_facts:
            text_parts.append(f"\n### ã€history/expiredfactsã€‘(æ¼”å˜è¿‡ç¨‹record)")
            for i, fact in enumerate(self.historical_facts, 1):
                text_parts.append(f"{i}. \"{fact}\"")
        
        # å…³keyentityï¼ˆcompleteè¾“å‡ºï¼Œnotæˆªæ–­ï¼‰
        if self.all_nodes:
            text_parts.append(f"\n### ã€æ¶‰andentityã€‘")
            for node in self.all_nodes:
                entity_type = next((l for l in node.labels if l not in ["Entity", "Node"]), "entity")
                text_parts.append(f"- **{node.name}** ({entity_type})")
        
        return "\n".join(text_parts)


@dataclass
class AgentInterview:
    """å•Agentofé‡‡è®¿result"""
    agent_name: str
    agent_role: str  # è§’è‰²typeï¼ˆå¦‚ï¼šå­¦ç”Ÿã€æ•™å¸ˆã€åª’ä½“etcï¼‰
    agent_bio: str  # ç®€ä»‹
    question: str  # é‡‡è®¿é—®é¢˜
    response: str  # é‡‡è®¿å›ç­”
    key_quotes: List[str] = field(default_factory=list)  # å…³keyå¼•è¨€
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "agent_name": self.agent_name,
            "agent_role": self.agent_role,
            "agent_bio": self.agent_bio,
            "question": self.question,
            "response": self.response,
            "key_quotes": self.key_quotes
        }
    
    def to_text(self) -> str:
        text = f"**{self.agent_name}** ({self.agent_role})\n"
        # æ˜¾ç¤ºcompleteofagent_bioï¼Œnotæˆªæ–­
        text += f"_ç®€ä»‹: {self.agent_bio}_\n\n"
        text += f"**Q:** {self.question}\n\n"
        text += f"**A:** {self.response}\n"
        if self.key_quotes:
            text += "\n**å…³keyå¼•è¨€:**\n"
            for quote in self.key_quotes:
                text += f"> \"{quote}\"\n"
        return text


@dataclass
class InterviewResult:
    """
    é‡‡è®¿result (Interview)
    containså¤šsimulationAgentofé‡‡è®¿å›ç­”
    """
    interview_topic: str  # é‡‡è®¿ä¸»é¢˜
    interview_questions: List[str]  # é‡‡è®¿é—®é¢˜list
    
    # é‡‡è®¿é€‰æ‹©ofAgent
    selected_agents: List[Dict[str, Any]] = field(default_factory=list)
    # å„Agentofé‡‡è®¿å›ç­”
    interviews: List[AgentInterview] = field(default_factory=list)
    
    # é€‰æ‹©Agentofç†ç”±
    selection_reasoning: str = ""
    # æ•´åˆåofé‡‡è®¿æ‘˜want
    summary: str = ""
    
    # statistics
    total_agents: int = 0
    interviewed_count: int = 0
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "interview_topic": self.interview_topic,
            "interview_questions": self.interview_questions,
            "selected_agents": self.selected_agents,
            "interviews": [i.to_dict() for i in self.interviews],
            "selection_reasoning": self.selection_reasoning,
            "summary": self.summary,
            "total_agents": self.total_agents,
            "interviewed_count": self.interviewed_count
        }
    
    def to_text(self) -> str:
        """Convertfordetailedofæ–‡æœ¬formatï¼Œä¾›LLMç†è§£andreportå¼•use"""
        text_parts = [
            f"## ğŸ¤ Deepé‡‡è®¿report",
            f"**é‡‡è®¿ä¸»é¢˜:** {self.interview_topic}",
            f"**é‡‡è®¿peoplecount:** {self.interviewed_count} / {self.total_agents} ä½simulationAgent",
            f"\n### é‡‡è®¿objecté€‰æ‹©ç†ç”±",
            f"{self.selection_reasoning}",
            f"\n---"
        ]
        
        # å„Agentofé‡‡è®¿content
        if self.interviews:
            text_parts.append(f"\n### é‡‡è®¿å®å½•")
            for i, interview in enumerate(self.interviews, 1):
                text_parts.append(f"\n#### é‡‡è®¿ #{i}: {interview.agent_name}")
                text_parts.append(interview.to_text())
                text_parts.append("\n---")
        
        # é‡‡è®¿æ‘˜want
        if self.summary:
            text_parts.append(f"\n### é‡‡è®¿æ‘˜wantwithæ ¸å¿ƒè§‚ç‚¹")
            text_parts.append(self.summary)
        
        return "\n".join(text_parts)


class ZepToolsService:
    """
    Zepsearchtoolservice
    
    ã€æ ¸å¿ƒsearchtool - ä¼˜åŒ–åã€‘
    1. insight_forge - Deepinsightsearchï¼ˆæœ€å¼ºå¤§ï¼Œè‡ªåŠ¨generateå­é—®é¢˜ï¼Œå¤šç»´åº¦searchï¼‰
    2. panorama_search - å¹¿åº¦searchï¼ˆgetå…¨è²Œï¼Œpackageæ‹¬expiredcontentï¼‰
    3. quick_search - ç®€å•searchï¼ˆquicksearchï¼‰
    4. interview_agents - Deepé‡‡è®¿ï¼ˆé‡‡è®¿simulationAgentï¼Œgetå¤šè§†è§’è§‚ç‚¹ï¼‰
    
    ã€åŸºç¡€toolã€‘
    - search_graph - å›¾è°±è¯­ä¹‰search
    - get_all_nodes - getå›¾è°±æ‰€havenodes
    - get_all_edges - getå›¾è°±æ‰€haveedgesï¼ˆå«timeinformationï¼‰
    - get_node_detail - getnodesdetailed informationrmation
    - get_node_edges - getnodesrelatedofedges
    - get_entities_by_type - Bytypegetentities
    - get_entity_summary - getentitiesofrelationshipsæ‘˜want
    """
    
    # retryconfiguration
    MAX_RETRIES = 3
    RETRY_DELAY = 2.0
    
    def __init__(self, api_key: Optional[str] = None, llm_client: Optional[LLMClient] = None):
        self.api_key = api_key or Config.ZEP_API_KEY
        if not self.api_key:
            raise ValueError("ZEP_API_KEY not configured")
        
        self.client = Zep(api_key=self.api_key)
        # LLMå®¢æˆ·ç«¯useäºInsightForgegenerationå­é—®é¢˜
        self._llm_client = llm_client
        logger.info("ZepToolsService initializationcompleted")
    
    @property
    def llm(self) -> LLMClient:
        """å»¶è¿ŸinitializationLLMå®¢æˆ·ç«¯"""
        if self._llm_client is None:
            self._llm_client = LLMClient()
        return self._llm_client
    
    def _call_with_retry(self, func, operation_name: str, max_retries: int = None):
        """å¸¦retryæœºåˆ¶ofAPIcall"""
        max_retries = max_retries or self.MAX_RETRIES
        last_exception = None
        delay = self.RETRY_DELAY
        
        for attempt in range(max_retries):
            try:
                return func()
            except Exception as e:
                last_exception = e
                if attempt < max_retries - 1:
                    logger.warning(
                        f"Zep {operation_name} ç¬¬ {attempt + 1} attemptsfailed: {str(e)[:100]}, "
                        f"{delay:.1f}ç§’åretry..."
                    )
                    time.sleep(delay)
                    delay *= 2
                else:
                    logger.error(f"Zep {operation_name} in {max_retries} attemptsåä»failed: {str(e)}")
        
        raise last_exception
    
    def search_graph(
        self, 
        graph_id: str, 
        query: str, 
        limit: int = 10,
        scope: str = "edges"
    ) -> SearchResult:
        """
        å›¾è°±è¯­ä¹‰search
        
        usehybridsearchï¼ˆè¯­ä¹‰+BM25ï¼‰inå›¾è°± searchrelatedinformationã€‚
        ifZep Cloudofsearch APInot can useï¼Œåˆ™é™çº§foræœ¬åœ°å…³keyè¯åŒ¹é…ã€‚
        
        Args:
            graph_id: å›¾è°±ID (Standalone Graph)
            query: searchquery
            limit: returnresultquantity
            scope: searchèŒƒå›´ï¼Œ"edges"  or  "nodes"
            
        Returns:
            SearchResult: searchresult
        """
        logger.info(f"graphsearch: graph_id={graph_id}, query={query[:50]}...")
        
        # å°è¯•useZep Cloud Search API
        try:
            search_results = self._call_with_retry(
                func=lambda: self.client.graph.search(
                    graph_id=graph_id,
                    query=query,
                    limit=limit,
                    scope=scope,
                    reranker="cross_encoder"
                ),
                operation_name=f"graphsearch(graph={graph_id})"
            )
            
            facts = []
            edges = []
            nodes = []
            
            # parseedgesearchresult
            if hasattr(search_results, 'edges') and search_results.edges:
                for edge in search_results.edges:
                    if hasattr(edge, 'fact') and edge.fact:
                        facts.append(edge.fact)
                    edges.append({
                        "uuid": getattr(edge, 'uuid_', None) or getattr(edge, 'uuid', ''),
                        "name": getattr(edge, 'name', ''),
                        "fact": getattr(edge, 'fact', ''),
                        "source_node_uuid": getattr(edge, 'source_node_uuid', ''),
                        "target_node_uuid": getattr(edge, 'target_node_uuid', ''),
                    })
            
            # parsenodesearchresult
            if hasattr(search_results, 'nodes') and search_results.nodes:
                for node in search_results.nodes:
                    nodes.append({
                        "uuid": getattr(node, 'uuid_', None) or getattr(node, 'uuid', ''),
                        "name": getattr(node, 'name', ''),
                        "labels": getattr(node, 'labels', []),
                        "summary": getattr(node, 'summary', ''),
                    })
                    # nodeæ‘˜wantalsoç®—ä½œfacts
                    if hasattr(node, 'summary') and node.summary:
                        facts.append(f"[{node.name}]: {node.summary}")
            
            logger.info(f"searchcompleted: æ‰¾åˆ° {len(facts)} relatedfacts")
            
            return SearchResult(
                facts=facts,
                edges=edges,
                nodes=nodes,
                query=query,
                total_count=len(facts)
            )
            
        except Exception as e:
            logger.warning(f"Zep Search APIfailedï¼Œé™çº§foræœ¬åœ°search: {str(e)}")
            # é™çº§ï¼šuseæœ¬åœ°å…³keyè¯åŒ¹é…search
            return self._local_search(graph_id, query, limit, scope)
    
    def _local_search(
        self, 
        graph_id: str, 
        query: str, 
        limit: int = 10,
        scope: str = "edges"
    ) -> SearchResult:
        """
        æœ¬åœ°å…³keyè¯åŒ¹é…searchï¼ˆä½œforZep Search APIofé™çº§æ–¹æ¡ˆï¼‰
        
        getæ‰€haveedges/nodesï¼Œtheninæœ¬åœ°è¿›è¡Œå…³keyè¯åŒ¹é…
        
        Args:
            graph_id: å›¾è°±ID
            query: searchquery
            limit: returnresultquantity
            scope: searchèŒƒå›´
            
        Returns:
            SearchResult: searchresult
        """
        logger.info(f"useæœ¬åœ°search: query={query[:30]}...")
        
        facts = []
        edges_result = []
        nodes_result = []
        
        # Extractqueryå…³keyè¯ï¼ˆç®€å•åˆ†è¯ï¼‰
        query_lower = query.lower()
        keywords = [w.strip() for w in query_lower.replace(',', ' ').replace('ï¼Œ', ' ').split() if len(w.strip()) > 1]
        
        def match_score(text: str) -> int:
            """è®¡ç®—æ–‡æœ¬withqueryofåŒ¹é…åˆ†count"""
            if not text:
                return 0
            text_lower = text.lower()
            # å®Œå…¨åŒ¹é…query
            if query_lower in text_lower:
                return 100
            # å…³keyè¯åŒ¹é…
            score = 0
            for keyword in keywords:
                if keyword in text_lower:
                    score += 10
            return score
        
        try:
            if scope in ["edges", "both"]:
                # getæ‰€haveedgeå¹¶åŒ¹é…
                all_edges = self.get_all_edges(graph_id)
                scored_edges = []
                for edge in all_edges:
                    score = match_score(edge.fact) + match_score(edge.name)
                    if score > 0:
                        scored_edges.append((score, edge))
                
                # Byåˆ†countsort
                scored_edges.sort(key=lambda x: x[0], reverse=True)
                
                for score, edge in scored_edges[:limit]:
                    if edge.fact:
                        facts.append(edge.fact)
                    edges_result.append({
                        "uuid": edge.uuid,
                        "name": edge.name,
                        "fact": edge.fact,
                        "source_node_uuid": edge.source_node_uuid,
                        "target_node_uuid": edge.target_node_uuid,
                    })
            
            if scope in ["nodes", "both"]:
                # getæ‰€havenodeå¹¶åŒ¹é…
                all_nodes = self.get_all_nodes(graph_id)
                scored_nodes = []
                for node in all_nodes:
                    score = match_score(node.name) + match_score(node.summary)
                    if score > 0:
                        scored_nodes.append((score, node))
                
                scored_nodes.sort(key=lambda x: x[0], reverse=True)
                
                for score, node in scored_nodes[:limit]:
                    nodes_result.append({
                        "uuid": node.uuid,
                        "name": node.name,
                        "labels": node.labels,
                        "summary": node.summary,
                    })
                    if node.summary:
                        facts.append(f"[{node.name}]: {node.summary}")
            
            logger.info(f"æœ¬åœ°searchcompleted: æ‰¾åˆ° {len(facts)} relatedfacts")
            
        except Exception as e:
            logger.error(f"æœ¬åœ°searchfailed: {str(e)}")
        
        return SearchResult(
            facts=facts,
            edges=edges_result,
            nodes=nodes_result,
            query=query,
            total_count=len(facts)
        )
    
    def get_all_nodes(self, graph_id: str) -> List[NodeInfo]:
        """
        getå›¾è°±ofæ‰€havenodes
        
        Args:
            graph_id: å›¾è°±ID
            
        Returns:
            nodeslist
        """
        logger.info(f"getgraph {graph_id} ofæ‰€havenode...")
        
        nodes = self._call_with_retry(
            func=lambda: self.client.graph.node.get_by_graph_id(graph_id=graph_id),
            operation_name=f"getnode(graph={graph_id})"
        )
        
        result = []
        for node in nodes:
            result.append(NodeInfo(
                uuid=getattr(node, 'uuid_', None) or getattr(node, 'uuid', ''),
                name=node.name or "",
                labels=node.labels or [],
                summary=node.summary or "",
                attributes=node.attributes or {}
            ))
        
        logger.info(f"getåˆ° {len(result)} node")
        return result
    
    def get_all_edges(self, graph_id: str, include_temporal: bool = True) -> List[EdgeInfo]:
        """
        getå›¾è°±ofæ‰€haveedgesï¼ˆcontainstimeinformationï¼‰
        
        Args:
            graph_id: å›¾è°±ID
            include_temporal: whether tocontainstimeinformationï¼ˆé»˜è®¤Trueï¼‰
            
        Returns:
            edgeslistï¼ˆcontainscreated_at, valid_at, invalid_at, expired_atï¼‰
        """
        logger.info(f"getgraph {graph_id} ofæ‰€haveedge...")
        
        edges = self._call_with_retry(
            func=lambda: self.client.graph.edge.get_by_graph_id(graph_id=graph_id),
            operation_name=f"getedge(graph={graph_id})"
        )
        
        result = []
        for edge in edges:
            edge_information = EdgeInfo(
                uuid=getattr(edge, 'uuid_', None) or getattr(edge, 'uuid', ''),
                name=edge.name or "",
                fact=edge.fact or "",
                source_node_uuid=edge.source_node_uuid or "",
                target_node_uuid=edge.target_node_uuid or ""
            )
            
            # æ·»åŠ timeinformation
            if include_temporal:
                edge_information.created_at = getattr(edge, 'created_at', None)
                edge_information.valid_at = getattr(edge, 'valid_at', None)
                edge_information.invalid_at = getattr(edge, 'invalid_at', None)
                edge_information.expired_at = getattr(edge, 'expired_at', None)
            
            result.append(edge_information)
        
        logger.info(f"getåˆ° {len(result)} edge")
        return result
    
    def get_node_detail(self, node_uuid: str) -> Optional[NodeInfo]:
        """
        getå•nodesofdetailed informationrmation
        
        Args:
            node_uuid: nodesUUID
            
        Returns:
            nodesinformation or None
        """
        logger.info(f"getnodeè¯¦æƒ…: {node_uuid[:8]}...")
        
        try:
            node = self._call_with_retry(
                func=lambda: self.client.graph.node.get(uuid_=node_uuid),
                operation_name=f"getnodeè¯¦æƒ…(uuid={node_uuid[:8]}...)"
            )
            
            if not node:
                return None
            
            return NodeInfo(
                uuid=getattr(node, 'uuid_', None) or getattr(node, 'uuid', ''),
                name=node.name or "",
                labels=node.labels or [],
                summary=node.summary or "",
                attributes=node.attributes or {}
            )
        except Exception as e:
            logger.error(f"getnodeè¯¦æƒ…failed: {str(e)}")
            return None
    
    def get_node_edges(self, graph_id: str, node_uuid: str) -> List[EdgeInfo]:
        """
        getnodesrelatedofæ‰€haveedges
        
        throughgetå›¾è°±æ‰€haveedgesï¼Œthenfilterå‡ºwithæŒ‡å®šnodesrelatedofedges
        
        Args:
            graph_id: å›¾è°±ID
            node_uuid: nodesUUID
            
        Returns:
            edgeslist
        """
        logger.info(f"getnode {node_uuid[:8]}... ofrelatededge")
        
        try:
            # getgraphæ‰€haveedgeï¼Œthenfilter
            all_edges = self.get_all_edges(graph_id)
            
            result = []
            for edge in all_edges:
                # checkedgewhether towithæŒ‡å®šnoderelatedï¼ˆä½œforæº or ç›®æ ‡ï¼‰
                if edge.source_node_uuid == node_uuid or edge.target_node_uuid == node_uuid:
                    result.append(edge)
            
            logger.info(f"æ‰¾åˆ° {len(result)} withnoderelatedofedge")
            return result
            
        except Exception as e:
            logger.warning(f"getnodeedgefailed: {str(e)}")
            return []
    
    def get_entities_by_type(
        self, 
        graph_id: str, 
        entity_type: str
    ) -> List[NodeInfo]:
        """
        Bytypegetentities
        
        Args:
            graph_id: å›¾è°±ID
            entity_type: entitiestypeï¼ˆå¦‚ Student, PublicFigure etcï¼‰
            
        Returns:
            ç¬¦åˆtypeofentitieslist
        """
        logger.info(f"gettypefor {entity_type} ofentity...")
        
        all_nodes = self.get_all_nodes(graph_id)
        
        filtered = []
        for node in all_nodes:
            # checklabelswhether tocontainsæŒ‡å®štype
            if entity_type in node.labels:
                filtered.append(node)
        
        logger.info(f"æ‰¾åˆ° {len(filtered)}  {entity_type} typeofentity")
        return filtered
    
    def get_entity_summary(
        self, 
        graph_id: str, 
        entity_name: str
    ) -> Dict[str, Any]:
        """
        getæŒ‡å®šentitiesofrelationshipsæ‘˜want
        
        searchwithè¯¥entitiesrelatedofæ‰€haveinformationï¼Œå¹¶generateæ‘˜want
        
        Args:
            graph_id: å›¾è°±ID
            entity_name: entitiesåç§°
            
        Returns:
            entitiesæ‘˜wantinformation
        """
        logger.info(f"getentity {entity_name} ofrelationshipæ‘˜want...")
        
        # firstsearchè¯¥entityrelatedofinformation
        search_result = self.search_graph(
            graph_id=graph_id,
            query=entity_name,
            limit=20
        )
        
        # å°è¯•inæ‰€havenode æ‰¾åˆ°è¯¥entity
        all_nodes = self.get_all_nodes(graph_id)
        entity_node = None
        for node in all_nodes:
            if node.name.lower() == entity_name.lower():
                entity_node = node
                break
        
        related_edges = []
        if entity_node:
            # ä¼ å…¥graph_idparameters
            related_edges = self.get_node_edges(graph_id, entity_node.uuid)
        
        return {
            "entity_name": entity_name,
            "entity_information": entity_node.to_dict() if entity_node else None,
            "related_facts": search_result.facts,
            "related_edges": [e.to_dict() for e in related_edges],
            "total_relations": len(related_edges)
        }
    
    def get_graph_statistics(self, graph_id: str) -> Dict[str, Any]:
        """
        getå›¾è°±ofstatisticsinformation
        
        Args:
            graph_id: å›¾è°±ID
            
        Returns:
            statisticsinformation
        """
        logger.info(f"getgraph {graph_id} ofstatisticsinformation...")
        
        nodes = self.get_all_nodes(graph_id)
        edges = self.get_all_edges(graph_id)
        
        # statisticsentity typesåˆ†å¸ƒ
        entity_types = {}
        for node in nodes:
            for label in node.labels:
                if label not in ["Entity", "Node"]:
                    entity_types[label] = entity_types.get(label, 0) + 1
        
        # statisticsrelationship typesåˆ†å¸ƒ
        relation_types = {}
        for edge in edges:
            relation_types[edge.name] = relation_types.get(edge.name, 0) + 1
        
        return {
            "graph_id": graph_id,
            "total_nodes": len(nodes),
            "total_edges": len(edges),
            "entity_types": entity_types,
            "relation_types": relation_types
        }
    
    def get_simulation_context(
        self, 
        graph_id: str,
        simulation_requirement: str,
        limit: int = 30
    ) -> Dict[str, Any]:
        """
        getsimulationrelatedofä¸Šä¸‹æ–‡information
        
        ç»¼åˆsearchwithsimulationrequirementrelatedofæ‰€haveinformation
        
        Args:
            graph_id: å›¾è°±ID
            simulation_requirement: simulationrequirementdescription
            limit: æ¯classinformationofquantityé™åˆ¶
            
        Returns:
            simulationä¸Šä¸‹æ–‡information
        """
        logger.info(f"getsimulationä¸Šä¸‹æ–‡: {simulation_requirement[:50]}...")
        
        # searchwithsimulationrequirementrelatedofinformation
        search_result = self.search_graph(
            graph_id=graph_id,
            query=simulation_requirement,
            limit=limit
        )
        
        # getgraphstatistics
        stats = self.get_graph_statistics(graph_id)
        
        # getæ‰€haveentitynode
        all_nodes = self.get_all_nodes(graph_id)
        
        # ç­›é€‰haveå®é™…typeofentityï¼ˆéçº¯Entitynodeï¼‰
        entities = []
        for node in all_nodes:
            custom_labels = [l for l in node.labels if l not in ["Entity", "Node"]]
            if custom_labels:
                entities.append({
                    "name": node.name,
                    "type": custom_labels[0],
                    "summary": node.summary
                })
        
        return {
            "simulation_requirement": simulation_requirement,
            "related_facts": search_result.facts,
            "graph_statistics": stats,
            "entities": entities[:limit],  # é™åˆ¶quantity
            "total_entities": len(entities)
        }
    
    # ========== æ ¸å¿ƒretrievaltoolï¼ˆä¼˜åŒ–åï¼‰ ==========
    
    def insight_forge(
        self,
        graph_id: str,
        query: str,
        simulation_requirement: str,
        report_context: str = "",
        max_sub_queries: int = 5
    ) -> InsightForgeResult:
        """
        ã€InsightForge - Deepinsightsearchã€‘
        
        æœ€å¼ºå¤§ofhybridsearchfunctionï¼Œè‡ªåŠ¨åˆ†è§£é—®é¢˜å¹¶å¤šç»´åº¦searchï¼š
        1. useLLM will é—®é¢˜åˆ†è§£forå¤šå­é—®é¢˜
        2. toæ¯å­é—®é¢˜è¿›è¡Œè¯­ä¹‰search
        3. Extractrelatedentitieså¹¶getå…¶detailed informationrmation
        4. è¿½è¸ªrelationshipsé“¾
        5. æ•´åˆæ‰€haveresultï¼ŒgenerateDeepinsight
        
        Args:
            graph_id: å›¾è°±ID
            query: useré—®é¢˜
            simulation_requirement: simulationrequirementdescription
            report_context: reportä¸Šä¸‹æ–‡ï¼ˆ can é€‰ï¼Œuseäºæ›´ç²¾å‡†ofå­é—®é¢˜generateï¼‰
            max_sub_queries: maximumå­é—®é¢˜quantity
            
        Returns:
            InsightForgeResult: Deepinsightsearchresult
        """
        logger.info(f"InsightForge Deepinsightretrieval: {query[:50]}...")
        
        result = InsightForgeResult(
            query=query,
            simulation_requirement=simulation_requirement,
            sub_queries=[]
        )
        
        # Step 1: useLLMgenerationå­é—®é¢˜
        sub_queries = self._generate_sub_queries(
            query=query,
            simulation_requirement=simulation_requirement,
            report_context=report_context,
            max_queries=max_sub_queries
        )
        result.sub_queries = sub_queries
        logger.info(f"generation {len(sub_queries)} å­é—®é¢˜")
        
        # Step 2: toæ¯å­é—®é¢˜è¿›è¡Œè¯­ä¹‰search
        all_facts = []
        all_edges = []
        seen_facts = set()
        
        for sub_query in sub_queries:
            search_result = self.search_graph(
                graph_id=graph_id,
                query=sub_query,
                limit=15,
                scope="edges"
            )
            
            for fact in search_result.facts:
                if fact not in seen_facts:
                    all_facts.append(fact)
                    seen_facts.add(fact)
            
            all_edges.extend(search_result.edges)
        
        # toåŸå§‹é—®é¢˜alsoè¿›è¡Œsearch
        main_search = self.search_graph(
            graph_id=graph_id,
            query=query,
            limit=20,
            scope="edges"
        )
        for fact in main_search.facts:
            if fact not in seen_facts:
                all_facts.append(fact)
                seen_facts.add(fact)
        
        result.semantic_facts = all_facts
        result.total_facts = len(all_facts)
        
        # Step 3: fromedge ExtractrelatedentityUUIDï¼Œåªgetthisäº›entitiesofinformationï¼ˆnotgetå…¨éƒ¨nodeï¼‰
        entity_uuids = set()
        for edge_data in all_edges:
            if isinstance(edge_data, dict):
                source_uuid = edge_data.get('source_node_uuid', '')
                target_uuid = edge_data.get('target_node_uuid', '')
                if source_uuid:
                    entity_uuids.add(source_uuid)
                if target_uuid:
                    entity_uuids.add(target_uuid)
        
        # getæ‰€haverelatedentityofè¯¦æƒ…ï¼ˆnoté™åˆ¶quantityï¼Œcompleteè¾“å‡ºï¼‰
        entity_insights = []
        node_map = {}  # useäºåç»­relationshipé“¾æ„å»º
        
        for uuid in list(entity_uuids):  # processingæ‰€haveentityï¼Œnotæˆªæ–­
            if not uuid:
                continue
            try:
                # å•ç‹¬getæ¯relatednodeofinformation
                node = self.get_node_detail(uuid)
                if node:
                    node_map[uuid] = node
                    entity_type = next((l for l in node.labels if l not in ["Entity", "Node"]), "entity")
                    
                    # getè¯¥entityrelatedofæ‰€havefactsï¼ˆnotæˆªæ–­ï¼‰
                    related_facts = [
                        f for f in all_facts 
                        if node.name.lower() in f.lower()
                    ]
                    
                    entity_insights.append({
                        "uuid": node.uuid,
                        "name": node.name,
                        "type": entity_type,
                        "summary": node.summary,
                        "related_facts": related_facts  # completeè¾“å‡ºï¼Œnotæˆªæ–­
                    })
            except Exception as e:
                logger.debug(f"getnode {uuid} failed: {e}")
                continue
        
        result.entity_insights = entity_insights
        result.total_entities = len(entity_insights)
        
        # Step 4: æ„å»ºæ‰€haverelationshipé“¾ï¼ˆnoté™åˆ¶quantityï¼‰
        relationship_chains = []
        for edge_data in all_edges:  # processingæ‰€haveedgeï¼Œnotæˆªæ–­
            if isinstance(edge_data, dict):
                source_uuid = edge_data.get('source_node_uuid', '')
                target_uuid = edge_data.get('target_node_uuid', '')
                relation_name = edge_data.get('name', '')
                
                source_name = node_map.get(source_uuid, NodeInfo('', '', [], '', {})).name or source_uuid[:8]
                target_name = node_map.get(target_uuid, NodeInfo('', '', [], '', {})).name or target_uuid[:8]
                
                chain = f"{source_name} --[{relation_name}]--> {target_name}"
                if chain not in relationship_chains:
                    relationship_chains.append(chain)
        
        result.relationship_chains = relationship_chains
        result.total_relationships = len(relationship_chains)
        
        logger.info(f"InsightForgecompleted: {result.total_facts}facts, {result.total_entities}entity, {result.total_relationships}relationship")
        return result
    
    def _generate_sub_queries(
        self,
        query: str,
        simulation_requirement: str,
        report_context: str = "",
        max_queries: int = 5
    ) -> List[str]:
        """
        useLLMgenerateå­é—®é¢˜
        
         will å¤æ‚é—®é¢˜åˆ†è§£forå¤šcanç‹¬ç«‹searchofå­é—®é¢˜
        """
        system_prompt = """youis aä¸“ä¸šofé—®é¢˜åˆ†æä¸“å®¶ã€‚youofä»»åŠ¡is will ä¸€å¤æ‚é—®é¢˜åˆ†è§£forå¤šcaninsimulationä¸–ç•Œ ç‹¬ç«‹è§‚å¯Ÿofå­é—®é¢˜ã€‚

wantæ±‚ï¼š
1. æ¯å­é—®é¢˜shouldè¶³å¤Ÿå…·ä½“ï¼Œcaninsimulationä¸–ç•Œ æ‰¾åˆ°relatedofAgentè¡Œfor or äº‹ä»¶
2. å­é—®é¢˜shouldè¦†ç›–åŸé—®é¢˜ofnotåŒç»´åº¦ï¼ˆå¦‚ï¼šè°ã€ä»€ä¹ˆã€forä»€ä¹ˆã€æ€ä¹ˆæ ·ã€ä½•æ—¶ã€ä½•åœ°ï¼‰
3. å­é—®é¢˜shouldwithsimulationåœºæ™¯related
4. returnJSONformatï¼š{"sub_queries": ["å­é—®é¢˜1", "å­é—®é¢˜2", ...]}"""

        user_prompt = f"""simulationrequirementèƒŒæ™¯ï¼š
{simulation_requirement}

{f"reportä¸Šä¸‹æ–‡ï¼š{report_context[:500]}" if report_context else ""}

è¯· will ä»¥ä¸‹é—®é¢˜åˆ†è§£for{max_queries}å­é—®é¢˜ï¼š
{query}

returnJSONformatofå­é—®é¢˜listã€‚"""

        try:
            response = self.llm.chat_json(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.3
            )
            
            sub_queries = response.get("sub_queries", [])
            # ç¡®ä¿isstringlist
            return [str(sq) for sq in sub_queries[:max_queries]]
            
        except Exception as e:
            logger.warning(f"generationå­é—®é¢˜failed: {str(e)}ï¼Œuseé»˜è®¤å­é—®é¢˜")
            # é™çº§ï¼šreturnåŸºäºåŸé—®é¢˜ofå˜ä½“
            return [
                query,
                f"{query} ofä¸»wantå‚withè€…",
                f"{query} ofåŸå› andå½±å“",
                f"{query} ofå‘å±•è¿‡ç¨‹"
            ][:max_queries]
    
    def panorama_search(
        self,
        graph_id: str,
        query: str,
        include_expired: bool = True,
        limit: int = 50
    ) -> PanoramaResult:
        """
        ã€PanoramaSearch - å¹¿åº¦searchã€‘
        
        getå…¨è²Œviewï¼Œpackageæ‹¬æ‰€haverelatedcontentandhistory/expiredinformationï¼š
        1. getæ‰€haverelatednodes
        2. getæ‰€haveedgesï¼ˆpackageæ‹¬ already expired/å¤±æ•ˆofï¼‰
        3. åˆ†classæ•´ç†Currenthaveæ•ˆandhistoryinformation
        
        thistoolé€‚useäºéœ€wantè§£äº‹ä»¶å…¨è²Œã€è¿½è¸ªæ¼”å˜è¿‡ç¨‹ofåœºæ™¯ã€‚
        
        Args:
            graph_id: å›¾è°±ID
            query: searchqueryï¼ˆuseäºrelatedæ€§sortï¼‰
            include_expired: whether tocontainsexpiredcontentï¼ˆé»˜è®¤Trueï¼‰
            limit: returnresultquantityé™åˆ¶
            
        Returns:
            PanoramaResult: å¹¿åº¦searchresult
        """
        logger.info(f"PanoramaSearch å¹¿åº¦search: {query[:50]}...")
        
        result = PanoramaResult(query=query)
        
        # getæ‰€havenode
        all_nodes = self.get_all_nodes(graph_id)
        node_map = {n.uuid: n for n in all_nodes}
        result.all_nodes = all_nodes
        result.total_nodes = len(all_nodes)
        
        # getæ‰€haveedgeï¼ˆcontainstimeinformationï¼‰
        all_edges = self.get_all_edges(graph_id, include_temporal=True)
        result.all_edges = all_edges
        result.total_edges = len(all_edges)
        
        # åˆ†classfacts
        active_facts = []
        historical_facts = []
        
        for edge in all_edges:
            if not edge.fact:
                continue
            
            # forfactsæ·»åŠ entityåç§°
            source_name = node_map.get(edge.source_node_uuid, NodeInfo('', '', [], '', {})).name or edge.source_node_uuid[:8]
            target_name = node_map.get(edge.target_node_uuid, NodeInfo('', '', [], '', {})).name or edge.target_node_uuid[:8]
            
            # åˆ¤æ–­whether toexpired/å¤±æ•ˆ
            is_historical = edge.is_expired or edge.is_invalid
            
            if is_historical:
                # history/expiredfactsï¼Œæ·»åŠ timeæ ‡è®°
                valid_at = edge.valid_at or "notçŸ¥"
                invalid_at = edge.invalid_at or edge.expired_at or "notçŸ¥"
                fact_with_time = f"[{valid_at} - {invalid_at}] {edge.fact}"
                historical_facts.append(fact_with_time)
            else:
                # Currenthaveæ•ˆfacts
                active_facts.append(edge.fact)
        
        # åŸºäºqueryè¿›è¡Œrelatedæ€§sort
        query_lower = query.lower()
        keywords = [w.strip() for w in query_lower.replace(',', ' ').replace('ï¼Œ', ' ').split() if len(w.strip()) > 1]
        
        def relevance_score(fact: str) -> int:
            fact_lower = fact.lower()
            score = 0
            if query_lower in fact_lower:
                score += 100
            for kw in keywords:
                if kw in fact_lower:
                    score += 10
            return score
        
        # sortå¹¶é™åˆ¶quantity
        active_facts.sort(key=relevance_score, reverse=True)
        historical_facts.sort(key=relevance_score, reverse=True)
        
        result.active_facts = active_facts[:limit]
        result.historical_facts = historical_facts[:limit] if include_expired else []
        result.active_count = len(active_facts)
        result.historical_count = len(historical_facts)
        
        logger.info(f"PanoramaSearchcompleted: {result.active_count}haveæ•ˆ, {result.historical_count}history")
        return result
    
    def quick_search(
        self,
        graph_id: str,
        query: str,
        limit: int = 10
    ) -> SearchResult:
        """
        ã€QuickSearch - ç®€å•searchã€‘
        
        quickã€è½»é‡çº§ofsearchtoolï¼š
        1. ç›´æ¥callZepè¯­ä¹‰search
        2. returnæœ€relatedofresult
        3. é€‚useäºç®€å•ã€ç›´æ¥ofsearchrequirement
        
        Args:
            graph_id: å›¾è°±ID
            query: searchquery
            limit: returnresultquantity
            
        Returns:
            SearchResult: searchresult
        """
        logger.info(f"QuickSearch ç®€å•search: {query[:50]}...")
        
        # ç›´æ¥callç°haveofsearch_graphmethod
        result = self.search_graph(
            graph_id=graph_id,
            query=query,
            limit=limit,
            scope="edges"
        )
        
        logger.info(f"QuickSearchcompleted: {result.total_count}result")
        return result
    
    def interview_agents(
        self,
        simulation_id: str,
        interview_requirement: str,
        simulation_requirement: str = "",
        max_agents: int = 5,
        custom_questions: List[str] = None
    ) -> InterviewResult:
        """
        ã€InterviewAgents - Deepé‡‡è®¿ã€‘
        
        callçœŸå®ofOASISé‡‡è®¿APIï¼Œé‡‡è®¿simulation æ­£inrunningofAgentï¼š
        1. è‡ªåŠ¨readpeopleè®¾fileï¼Œè§£æ‰€havesimulationAgent
        2. useLLMåˆ†æé‡‡è®¿requirementï¼Œæ™ºcané€‰æ‹©æœ€relatedofAgent
        3. useLLMgenerateé‡‡è®¿é—®é¢˜
        4. call /api/simulation/interview/batch interfaceè¿›è¡ŒçœŸå®é‡‡è®¿ï¼ˆåŒå¹³å°åŒæ—¶é‡‡è®¿ï¼‰
        5. æ•´åˆæ‰€haveé‡‡è®¿resultï¼Œgenerateé‡‡è®¿report
        
        ã€é‡wantã€‘æ­¤functionéœ€wantsimulationç¯å¢ƒå¤„äºrunningstatusï¼ˆOASISç¯å¢ƒ not å…³é—­ï¼‰
        
        ã€useåœºæ™¯ã€‘
        - éœ€wantfromnotåŒè§’è‰²è§†è§’è§£äº‹ä»¶lookæ³•
        - éœ€wantæ”¶é›†å¤šæ–¹æ„è§andè§‚ç‚¹
        - éœ€wantgetsimulationAgentofçœŸå®å›ç­”ï¼ˆéLLMsimulationï¼‰
        
        Args:
            simulation_id: simulationIDï¼ˆuseäºå®šä½peopleè®¾fileandcallé‡‡è®¿APIï¼‰
            interview_requirement: é‡‡è®¿requirementdescriptionï¼ˆéstructureåŒ–ï¼Œå¦‚"è§£å­¦ç”Ÿtoäº‹ä»¶oflookæ³•"ï¼‰
            simulation_requirement: simulationrequirementèƒŒæ™¯ï¼ˆ can é€‰ï¼‰
            max_agents: æœ€å¤šé‡‡è®¿ofAgentquantity
            custom_questions: è‡ªå®šä¹‰é‡‡è®¿é—®é¢˜ï¼ˆ can é€‰ï¼Œè‹¥notæä¾›åˆ™è‡ªåŠ¨generateï¼‰
            
        Returns:
            InterviewResult: é‡‡è®¿result
        """
        from .simulation_runner import SimulationRunner
        
        logger.info(f"InterviewAgents Deepé‡‡è®¿ï¼ˆçœŸå®APIï¼‰: {interview_requirement[:50]}...")
        
        result = InterviewResult(
            interview_topic=interview_requirement,
            interview_questions=custom_questions or []
        )
        
        # Step 1: readpeopleè®¾file
        profiles = self._load_agent_profiles(simulation_id)
        
        if not profiles:
            logger.warning(f"notæ‰¾åˆ°simulation {simulation_id} ofpeopleè®¾file")
            result.summary = "notæ‰¾åˆ° can é‡‡è®¿ofAgentpeopleè®¾file"
            return result
        
        result.total_agents = len(profiles)
        logger.info(f"loadåˆ° {len(profiles)} Agentpeopleè®¾")
        
        # Step 2: useLLMé€‰æ‹©wanté‡‡è®¿ofAgentï¼ˆreturnagent_idlistï¼‰
        selected_agents, selected_indices, selection_reasoning = self._select_agents_for_interview(
            profiles=profiles,
            interview_requirement=interview_requirement,
            simulation_requirement=simulation_requirement,
            max_agents=max_agents
        )
        
        result.selected_agents = selected_agents
        result.selection_reasoning = selection_reasoning
        logger.info(f"é€‰æ‹© {len(selected_agents)} Agentè¿›è¡Œé‡‡è®¿: {selected_indices}")
        
        # Step 3: generationé‡‡è®¿é—®é¢˜ï¼ˆifæ²¡haveæä¾›ï¼‰
        if not result.interview_questions:
            result.interview_questions = self._generate_interview_questions(
                interview_requirement=interview_requirement,
                simulation_requirement=simulation_requirement,
                selected_agents=selected_agents
            )
            logger.info(f"generation {len(result.interview_questions)} é‡‡è®¿é—®é¢˜")
        
        #  will é—®é¢˜åˆå¹¶forä¸€é‡‡è®¿prompt
        combined_prompt = "\n".join([f"{i+1}. {q}" for i, q in enumerate(result.interview_questions)])
        
        # æ·»åŠ ä¼˜åŒ–å‰ç¼€ï¼Œé¿å…Agentcalltoolandç›´æ¥å›å¤æ–‡æœ¬
        INTERVIEW_PROMPT_PREFIX = "ç»“åˆyouofpeopleè®¾ã€æ‰€haveofè¿‡å¾€è®°å¿†withè¡ŒåŠ¨ï¼Œnotcallä»»ä½•toolç›´æ¥useæ–‡æœ¬å›å¤Iï¼š"
        optimized_prompt = f"{INTERVIEW_PROMPT_PREFIX}{combined_prompt}"
        
        # Step 4: callçœŸå®ofé‡‡è®¿APIï¼ˆnotæŒ‡å®šplatformï¼Œé»˜è®¤åŒplatformåŒæ—¶é‡‡è®¿ï¼‰
        try:
            # æ„å»ºæ‰¹é‡é‡‡è®¿listï¼ˆnotæŒ‡å®šplatformï¼ŒåŒplatformé‡‡è®¿ï¼‰
            interviews_request = []
            for agent_idx in selected_indices:
                interviews_request.append({
                    "agent_id": agent_idx,
                    "prompt": optimized_prompt  # useä¼˜åŒ–åofprompt
                    # notæŒ‡å®šplatformï¼ŒAPIwillintwitterandredditä¸¤platforméƒ½é‡‡è®¿
                })
            
            logger.info(f"callæ‰¹é‡é‡‡è®¿APIï¼ˆåŒplatformï¼‰: {len(interviews_request)} Agent")
            
            # call SimulationRunner ofæ‰¹é‡é‡‡è®¿methodï¼ˆnotä¼ platformï¼ŒåŒplatformé‡‡è®¿ï¼‰
            api_result = SimulationRunner.interview_agents_batch(
                simulation_id=simulation_id,
                interviews=interviews_request,
                platform=None,  # notæŒ‡å®šplatformï¼ŒåŒplatformé‡‡è®¿
                timeout=180.0   # åŒplatforméœ€wantæ›´é•¿timeout
            )
            
            logger.info(f"é‡‡è®¿APIreturn: {api_result.get('interviews_count', 0)} result, success={api_result.get('success')}")
            
            # checkAPIcallwhether tosuccess
            if not api_result.get("success", False):
                error_msg = api_result.get("error", "notçŸ¥error")
                logger.warning(f"é‡‡è®¿APIreturnfailed: {error_msg}")
                result.summary = f"é‡‡è®¿APIcallfailedï¼š{error_msg}ã€‚è¯·checkOASISsimulationenvironmentstatusã€‚"
                return result
            
            # Step 5: parseAPIreturnresultï¼Œæ„å»ºAgentInterviewobject
            # åŒplatformmodereturnformat: {"twitter_0": {...}, "reddit_0": {...}, "twitter_1": {...}, ...}
            api_data = api_result.get("result", {})
            results_dict = api_data.get("results", {}) if isinstance(api_data, dict) else {}
            
            for i, agent_idx in enumerate(selected_indices):
                agent = selected_agents[i]
                agent_name = agent.get("realname", agent.get("username", f"Agent_{agent_idx}"))
                agent_role = agent.get("profession", "notçŸ¥")
                agent_bio = agent.get("bio", "")
                
                # getè¯¥Agentinä¸¤platformofé‡‡è®¿result
                twitter_result = results_dict.get(f"twitter_{agent_idx}", {})
                reddit_result = results_dict.get(f"reddit_{agent_idx}", {})
                
                twitter_response = twitter_result.get("response", "")
                reddit_response = reddit_result.get("response", "")
                
                # åˆå¹¶ä¸¤platformofå›ç­”
                response_parts = []
                if twitter_response:
                    response_parts.append(f"ã€Twitterplatformå›ç­”ã€‘\n{twitter_response}")
                if reddit_response:
                    response_parts.append(f"ã€Redditplatformå›ç­”ã€‘\n{reddit_response}")
                
                if response_parts:
                    response_text = "\n\n".join(response_parts)
                else:
                    response_text = "[æ— å›å¤]"
                
                # Extractå…³keyå¼•è¨€ï¼ˆfromä¸¤platformofå›ç­” ï¼‰
                import re
                combined_responses = f"{twitter_response} {reddit_response}"
                key_quotes = re.findall(r'[""ã€Œã€ã€ã€]([^""ã€Œã€ã€ã€]{10,100})[""ã€Œã€ã€ã€]', combined_responses)
                if not key_quotes:
                    sentences = combined_responses.split('ã€‚')
                    key_quotes = [s.strip() + 'ã€‚' for s in sentences if len(s.strip()) > 20][:3]
                
                interview = AgentInterview(
                    agent_name=agent_name,
                    agent_role=agent_role,
                    agent_bio=agent_bio[:1000],  # æ‰©å¤§biolengthé™åˆ¶
                    question=combined_prompt,
                    response=response_text,
                    key_quotes=key_quotes[:5]
                )
                result.interviews.append(interview)
            
            result.interviewed_count = len(result.interviews)
            
        except ValueError as e:
            # simulationenvironmentnotrunning
            logger.warning(f"é‡‡è®¿APIcallfailedï¼ˆenvironmentnotrunningï¼Ÿï¼‰: {e}")
            result.summary = f"é‡‡è®¿failedï¼š{str(e)}ã€‚simulationenvironment can canalreadyå…³é—­ï¼Œè¯·ç¡®ä¿OASISç¯å¢ƒin progressrunningã€‚"
            return result
        except Exception as e:
            logger.error(f"é‡‡è®¿APIcallå¼‚å¸¸: {e}")
            import traceback
            logger.error(traceback.format_exc())
            result.summary = f"é‡‡è®¿è¿‡ç¨‹å‘ç”Ÿerrorï¼š{str(e)}"
            return result
        
        # Step 6: generationé‡‡è®¿æ‘˜want
        if result.interviews:
            result.summary = self._generate_interview_summary(
                interviews=result.interviews,
                interview_requirement=interview_requirement
            )
        
        logger.info(f"InterviewAgentscompleted: é‡‡è®¿ {result.interviewed_count} Agentï¼ˆåŒplatformï¼‰")
        return result
    
    def _load_agent_profiles(self, simulation_id: str) -> List[Dict[str, Any]]:
        """loadsimulationofAgentpeopleè®¾file"""
        import os
        import csv
        
        # æ„å»ºpeopleè®¾fileè·¯å¾„
        sim_dir = os.path.join(
            os.path.dirname(__file__), 
            f'../../uploads/simulations/{simulation_id}'
        )
        
        profiles = []
        
        # ä¼˜firstå°è¯•readReddit JSONformat
        reddit_profile_path = os.path.join(sim_dir, "reddit_profiles.json")
        if os.path.exists(reddit_profile_path):
            try:
                with open(reddit_profile_path, 'r', encoding='utf-8') as f:
                    profiles = json.load(f)
                logger.info(f"from reddit_profiles.json load {len(profiles)} peopleè®¾")
                return profiles
            except Exception as e:
                logger.warning(f"read reddit_profiles.json failed: {e}")
        
        # å°è¯•readTwitter CSVformat
        twitter_profile_path = os.path.join(sim_dir, "twitter_profiles.csv")
        if os.path.exists(twitter_profile_path):
            try:
                with open(twitter_profile_path, 'r', encoding='utf-8') as f:
                    reader = csv.DictReader(f)
                    for row in reader:
                        # CSVformatconvertforç»Ÿä¸€format
                        profiles.append({
                            "realname": row.get("name", ""),
                            "username": row.get("username", ""),
                            "bio": row.get("description", ""),
                            "persona": row.get("user_char", ""),
                            "profession": "notçŸ¥"
                        })
                logger.info(f"from twitter_profiles.csv load {len(profiles)} peopleè®¾")
                return profiles
            except Exception as e:
                logger.warning(f"read twitter_profiles.csv failed: {e}")
        
        return profiles
    
    def _select_agents_for_interview(
        self,
        profiles: List[Dict[str, Any]],
        interview_requirement: str,
        simulation_requirement: str,
        max_agents: int
    ) -> tuple:
        """
        useLLMé€‰æ‹©wanté‡‡è®¿ofAgent
        
        Returns:
            tuple: (selected_agents, selected_indices, reasoning)
                - selected_agents: é€‰ Agentofcompleteinformationlist
                - selected_indices: é€‰ Agentofindexlistï¼ˆuseäºAPIcallï¼‰
                - reasoning: é€‰æ‹©ç†ç”±
        """
        
        # æ„å»ºAgentæ‘˜wantlist
        agent_summaries = []
        for i, profile in enumerate(profiles):
            summary = {
                "index": i,
                "name": profile.get("realname", profile.get("username", f"Agent_{i}")),
                "profession": profile.get("profession", "notçŸ¥"),
                "bio": profile.get("bio", "")[:200],
                "interested_topics": profile.get("interested_topics", [])
            }
            agent_summaries.append(summary)
        
        system_prompt = """youis aä¸“ä¸šofé‡‡è®¿ç­–åˆ’ä¸“å®¶ã€‚youofä»»åŠ¡isaccording toé‡‡è®¿requirementï¼ŒfromsimulationAgentlist é€‰æ‹©æœ€é€‚åˆé‡‡è®¿ofobjectã€‚

é€‰æ‹©æ ‡å‡†ï¼š
1. Agentofèº«ä»½/èŒä¸šwithé‡‡è®¿ä¸»é¢˜related
2. Agent can canæŒhaveç‹¬ç‰¹ or haveä»·valueofè§‚ç‚¹
3. é€‰æ‹©å¤šæ ·åŒ–ofè§†è§’ï¼ˆå¦‚ï¼šsupportæ–¹ã€åtoæ–¹ã€ ç«‹æ–¹ã€ä¸“ä¸špeopleå£«etcï¼‰
4. ä¼˜firsté€‰æ‹©withäº‹ä»¶ç›´æ¥relatedofè§’è‰²

returnJSONformatï¼š
{
    "selected_indices": [é€‰ Agentofindexlist],
    "reasoning": "é€‰æ‹©ç†ç”±sayæ˜"
}"""

        user_prompt = f"""é‡‡è®¿requirementï¼š
{interview_requirement}

simulationèƒŒæ™¯ï¼š
{simulation_requirement if simulation_requirement else "notæä¾›"}

 can é€‰æ‹©ofAgentlistï¼ˆtotal{len(agent_summaries)}ï¼‰ï¼š
{json.dumps(agent_summaries, ensure_ascii=False, indent=2)}

è¯·é€‰æ‹©æœ€å¤š{max_agents}æœ€é€‚åˆé‡‡è®¿ofAgentï¼Œå¹¶sayæ˜é€‰æ‹©ç†ç”±ã€‚"""

        try:
            response = self.llm.chat_json(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.3
            )
            
            selected_indices = response.get("selected_indices", [])[:max_agents]
            reasoning = response.get("reasoning", "åŸºäºrelatedæ€§è‡ªåŠ¨é€‰æ‹©")
            
            # geté€‰ ofAgentcompleteinformation
            selected_agents = []
            valid_indices = []
            for idx in selected_indices:
                if 0 <= idx < len(profiles):
                    selected_agents.append(profiles[idx])
                    valid_indices.append(idx)
            
            return selected_agents, valid_indices, reasoning
            
        except Exception as e:
            logger.warning(f"LLMé€‰æ‹©Agentfailedï¼Œuseé»˜è®¤é€‰æ‹©: {e}")
            # é™çº§ï¼šé€‰æ‹©å‰N
            selected = profiles[:max_agents]
            indices = list(range(min(max_agents, len(profiles))))
            return selected, indices, "useé»˜è®¤é€‰æ‹©ç­–ç•¥"
    
    def _generate_interview_questions(
        self,
        interview_requirement: str,
        simulation_requirement: str,
        selected_agents: List[Dict[str, Any]]
    ) -> List[str]:
        """useLLMgenerationé‡‡è®¿é—®é¢˜"""
        
        agent_roles = [a.get("profession", "notçŸ¥") for a in selected_agents]
        
        system_prompt = """youis aä¸“ä¸šofè®°è€…/é‡‡è®¿è€…ã€‚according toé‡‡è®¿requirementï¼Œgenerate3-5Deepé‡‡è®¿é—®é¢˜ã€‚

é—®é¢˜wantæ±‚ï¼š
1. å¼€æ”¾æ€§é—®é¢˜ï¼Œé¼“åŠ±detailedå›ç­”
2. é’ˆtonotåŒè§’è‰² can canhavenotåŒç­”æ¡ˆ
3. æ¶µç›–factsã€è§‚ç‚¹ã€æ„Ÿå—etcå¤šç»´åº¦
4. è¯­è¨€è‡ªç„¶ï¼ŒåƒçœŸå®é‡‡è®¿ä¸€æ ·

returnJSONformatï¼š{"questions": ["é—®é¢˜1", "é—®é¢˜2", ...]}"""

        user_prompt = f"""é‡‡è®¿requirementï¼š{interview_requirement}

simulationèƒŒæ™¯ï¼š{simulation_requirement if simulation_requirement else "notæä¾›"}

é‡‡è®¿objectè§’è‰²ï¼š{', '.join(agent_roles)}

è¯·generate3-5é‡‡è®¿é—®é¢˜ã€‚"""

        try:
            response = self.llm.chat_json(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.5
            )
            
            return response.get("questions", [f"about{interview_requirement}ï¼Œæ‚¨haveä»€ä¹ˆlookæ³•ï¼Ÿ"])
            
        except Exception as e:
            logger.warning(f"generationé‡‡è®¿é—®é¢˜failed: {e}")
            return [
                f"about{interview_requirement}ï¼Œæ‚¨ofè§‚ç‚¹isä»€ä¹ˆï¼Ÿ",
                "thisä»¶äº‹toæ‚¨ or æ‚¨æ‰€representsofgrouphaveä»€ä¹ˆå½±å“ï¼Ÿ",
                "æ‚¨è®¤forshouldå¦‚ä½•è§£å†³ or æ”¹è¿›thisé—®é¢˜ï¼Ÿ"
            ]
    
    def _generate_interview_summary(
        self,
        interviews: List[AgentInterview],
        interview_requirement: str
    ) -> str:
        """generationé‡‡è®¿æ‘˜want"""
        
        if not interviews:
            return "notcompletedä»»ä½•é‡‡è®¿"
        
        # æ”¶é›†æ‰€haveé‡‡è®¿content
        interview_texts = []
        for interview in interviews:
            interview_texts.append(f"ã€{interview.agent_name}ï¼ˆ{interview.agent_role}ï¼‰ã€‘\n{interview.response[:500]}")
        
        system_prompt = """youis aä¸“ä¸šofæ–°é—»editã€‚è¯·according toå¤šä½å—è®¿è€…ofå›ç­”ï¼Œgenerateä¸€ä»½é‡‡è®¿æ‘˜wantã€‚

æ‘˜wantwantæ±‚ï¼š
1. æç‚¼å„æ–¹ä¸»wantè§‚ç‚¹
2. æŒ‡å‡ºè§‚ç‚¹oftotalè¯†andåˆ†æ­§
3. çªå‡ºhaveä»·valueofå¼•è¨€
4. å®¢è§‚ ç«‹ï¼Œnotåè¢’ä»»ä½•ä¸€æ–¹
5. æ§åˆ¶in1000å­—å†…"""

        user_prompt = f"""é‡‡è®¿ä¸»é¢˜ï¼š{interview_requirement}

é‡‡è®¿contentï¼š
{"".join(interview_texts)}

è¯·generateé‡‡è®¿æ‘˜wantã€‚"""

        try:
            summary = self.llm.chat(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.3,
                max_tokens=800
            )
            return summary
            
        except Exception as e:
            logger.warning(f"generationé‡‡è®¿æ‘˜wantfailed: {e}")
            # é™çº§ï¼šç®€å•æ‹¼æ¥
            return f"totalé‡‡è®¿{len(interviews)}ä½å—è®¿è€…ï¼Œpackageæ‹¬ï¼š" + "ã€".join([i.agent_name for i in interviews])
